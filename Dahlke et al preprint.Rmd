---
title: "PIE Metrics: Quantifying the Systematic Bias in the Ephemerality and Inaccessibility of Web Scraping Content from URL-Logged Web-Browsing Digital Trace Data"
author: 
- Ross Dahlke ^1,2^
- Deepak Kumar ^1,3^
- Zakir Durumeric ^1,3^
- Jeffrey T. Hancock ^1,2^
date: \scriptsize^1^ Stanford University ^2^ Department of Communication ^3^ Department of Computer Science
#date: "`r format(Sys.time(), '%B %d, %Y')`"
output: 
  pdf_document: default
header-includes:
    - \usepackage{setspace}\doublespacing
indent: true
bibliography: bibliography.bib
csl: apa.csl
biblio-style: apa
code_folding: hide
abstract: "Social scientists and computer scientists are increasingly using observational digital trace data and analyzing these data post hoc to understand the content people are exposed to online. However, these content collection efforts may be systematically biased when the entirety of the data cannot be captured  retroactively. We call this unstated assumption the problematic assumption of persistence. To examine the extent to which this assumption may exist, we examine over 21 million URL-logged web browser visits from 1,515 participants over four months and record the degree to which hard news and misinformation URLs individuals visited were persistent, inaccessible, or ephemeral. While we find that the URLs collected are largely persistent, we find there are systematic biases in which URLs are ephemeral and inaccessible. For example, conservative misinformation URLs are more likely to be ephemeral than other types of misinformation. To standardize the reporting and understanding of the problematic assumption of persistence, we offer a set of metrics, $PersistenceRate$, $InaccessibilityRate$, $EphemeralityRate$ ($PIE$ metrics), that future research should report when using digital trace and web scraping data."
---

```{r include = F}
library(tidyverse)
```

# Introduction

Social science researchers are increasingly turning to observational web-tracking and digital trace data to understand patterns of exposure and effects of digital content. However, most social scientists do not collect digital trace data in real-time but instead retroactively try to access them, often through an API [Application Programming Interface, @junger2021brief; @praet2022s], data vendor [e.g., @lyons2022we], or scraping the content of web pages [@freelon2018computational]. In the present work, we focus on this post hoc scraping of the content of web pages, a common practice among researchers [e.g., @ben2016does; @guess2021almost; @guess2021consequences; @li2021trails; @reiss2022dissecting; @wojcieszak2021no]. These post-facto content collection efforts, however, may be systematically biased by the inability to post hoc capture the content of many of these websites. For example, a website may have been deleted or behind a paywall. How much of the content digital scholars seek to analyze is ephemeral, which we define as content that computers can no longer connect to (i.e., the DNS records are missing), or inaccessible, which refers to content that no longer exists at the collected URLs (i.e., the website returns a 404 error or is behind a paywall)? Furthermore, why is some content ephemeral or inaccessible and others persistent? 

To answer these questions, we leverage a dataset of 21 million URLs visited by a panel of 1,515 American adults to quantify the operative status–persistent, ephemeral, inaccessible–of hard news and misinformation web visits. We scraped each hard news and misinformation URL a participant visited via a fully-fledged web browser (e.g., Google Chrome) to capture the content loaded on the page. In our paper, we make three contributions: First, we estimate the levels of persistence, ephemerality, and inaccessibility of web pages that interest scholars studying digital behavioral data. Second, we investigate systematic differences in ephemeral content versus persistent web pages and show discrepancies across ideological content. Third, we investigate why this content is ephemeral and inaccessible. One possibility is that website content may expire due to financial constraints; another is that websites may adversarially “hide” content to avoid automatic detection (e.g., web cloaking). Ultimately, we recommend that scholars adopt a standardized set of reporting metrics that we call the $PIE$ metrics ($PeristenceRate$, $InaccessibilityRate$, and $EphemeralityRate$) and a reporting format that researchers using web scraping can take to standardize reporting of potential systematic biases in their data. 

The proliferation of digital trace data [@baumgartner2022novel; @choi2020digital; @jungherr2017digital; @kreuter2020collecting; @revilla2017using] has led to a “Big Data” revolution [@chen2020big; @christ2021big; @eck2021big; @gil2017citizenship; @wells2017combining]. Now, social scientists can explore new questions in human behavior that were difficult or impossible to study in the past. For example,  recent research has examined the relationship between political interest and the actual sharing of political information on social media [@haenschen2020self], gendered differences in civic engagement [@brandtzaeg2017facebook], digital behaviors and vote choice [@bach2021predicting], and observed digital news consumption [@moller2020explaining]. 

Most of these data are collected post hoc and, therefore, can be studied because they are *persistent*. In the field of communication, persistent communication is permanent, static, and atemporal [@linell2004written, p. 8]. Scholars can revisit preserved written text indefinitely, which has led to the exponential growth of big data forms of research in the social sciences. However, these data are prone to biases that social science researchers need to grapple with, for example, considering whose data is not being recorded and, thus, analyzed [@hargittai2020potential]. More obviously, computational social science research that relies on digital trace data may be conducted only on persistent data because possibly only persistent media are being recorded. If specific media leave no digital trace but nonetheless play a role in people’s experiences, these omissions could have consequences on all types of studies of digital trace data. Another issue is that just because digital trace data is persistent does not necessarily mean that it is accessible to researchers. 

We call the reliance on digital trace data in computational social science the *problematic assumption of persistence*. This assumption is often unstated but assumes that the digital traces available to a researcher are representative and complete. We argue here that while a great deal of digital trace data is persistent and reasonably captures social behavior or experiences, there are also trace data that are ephemeral and inaccessible. Below we lay out these two other forms of trace data that we argue may undermine assumptions that trace data are representative and complete.

## Ephemeral Communication

Despite the problematic assumption of persistence, communication scholars have long argued that human communication exists in one of two states: persistent or ephemeral [e.g., @clark1996using; @linell2004written]. In contrast to “atemporal” persistent communication, ephemeral communication is fleeting and ceases to exist; it is “distributed in time” [@linell2004written, p. 5]. For example, spoken word, if unrecorded, leaves no tangible evidence of its prior existence and contents. Modern media technology complicates the relationship between persistence and ephemerality. Instagram stories [@bainotti2021archive; @carah2016brands; @vazquez2019ephemeral] and Snapchat [@bayer2016sharing; @cavalcanti2017media; @chowdhury2021ceam; @mcroberts2019behind; @villaespesa2020ephemeral] are two prominent contemporary media platforms that feature ephemeral content. These platforms are designed to disappear after a specific amount of time, generally 24 hours. Given the fleeting nature of these communications, these ephemeral media model the oral paradigm of communication and storytelling [@soffer2016oral], but they introduce a new dynamic of easy capture where they are designed to be ephemeral but can be captured, for example, through screenshots on personal devices.

Early internet scholars documented the extent to which web pages were persistent or ephemeral. For example, early estimates found that websites are generally persistent, with about 17.2% of web pages being ephemeral [@koehler1999analysis]. This line of inquiry has also been extended to academic publications. "Citation rot" or "link rot" is when digital academic article reference material becomes unretrievable [@tyler2003librarians] and potentially disrupts scholarly progress because scholars cannot find relevant reference material. This concern continues today [e.g., @klein2014scholarly; @d2015urls; @perkel2015trouble] and is shared across disciplines, for example, in communication [e.g., @dimitrova2007half; @spence2020retrieving] and political science [e.g., @gertler2017reference]. Persistence is important to scholars because it allows for the recreation and revisitation of the original content that scholars desire to study.

In the computer science security community, significant prior work has studied the ways in which adversarial actors cloak or hide malicious activity using Fast Flux Domains [@holz2008measuring]. These ephemeral domains are brought online for a short time, typically to conduct some kind of internet abuse (e.g., distributed denial-of-service attacks or DDoS), and quickly taken offline to avoid discovery. Studying the structure of these domains is key to understanding how botnets propagate [@stone2009your; @bilge2011exposure] and can inform defenses against abusive Internet behaviors [@perdisci2018method].

## Inaccessible Communication 

Past social science scholarship has considered the states of persistence and ephemerality of data and their implications for research. However, we argue that a third state of digital trace data is also important to computational social science: inaccessible data. Inaccessible data are not ephemeral in the sense that they continue to exist, but they are not fully persistent because they are not easily accessible. For example, paywall journalism creates communication that are often inaccessible. Paywalls are barriers between internet users and online content from news organizations [@pickard2014salvation]. The news publishing industry quickly adopted [@franklin2014future] this “retro-innovation” [@arrese2016gratis] in an effort to find new revenue streams [@pavlik2013innovation; @sjovaag2016introducing] with mixed success [@myllylahti2014newspaper]. Journalistic stories behind paywalls continue to exist and are visitable, so they are not ephemeral. However, one must possess proper credentials to access the content–not just anyone can visit the content in the first place. In other words, this content is inaccessible. 

This in-between state of inaccessible communication, persistent but not accessible, is often under-considered. News organizations do not randomly construct paywalls; thus, content is not randomly inaccessible to people, including researchers. For example, even on the same website, hard news and opinion pieces are more likely to be behind paywalls than other web pages [@myllylahti2017content]--the sort of content most likely to be of interest to scholars. In addition, news organizations will occasionally temporarily drop their paywall for public emergencies, planned special events, and broader access for civically valuable content [@ananny2016drop]. 

Of course, inaccessible data are not new. For example, one may have had to pay for print newspapers. What is new, however, is how researchers are attempting to access the data. While researchers in the past may have accessed the totality of news that appeared in The New York Times via a first- or third-party archive, researchers are increasingly collecting their own data, often through web scraping [@krotov2018legality; @landers2016primer; @olmedilla2016harvesting]. Thus, inaccessible data pose additional problems for researchers above and beyond ephemerality because scholars must also consider how to access the content in addition to simply recording their existence. For example, internet scholars can simply record a webpage snapshot before the page gets taken down and becomes ephemeral. Researchers must also decide how to access the web page’s contents in addition to the capture step for inaccessible pages. 


## Persistence, Inaccessibility, Ephemerality, and the Study of Misinformation

In the present paper, we examine the inaccessibility relative to ephemerality and persistence in the context of misinformation. The study of misinformation on the internet has become an important area of research that relies on digital trace data. Many studies examine how often and in what ways people are exposed to misinformation online [@dahlke2022mixed; @guess2020exposure; @moore2022exposure] and to what effect [@dahlke2022effect]. One concern in misinformation research is that it has not accounted for ephemeral and inaccessible web-based misinformation. Many popular misinformation studies leverage lists of curated misinformation websites, but these websites are often unavailable or offline by the time studies are conducted [@han2022infrastructure; @hanley2022no; @hounsel2020identifying]. Internet measurement studies on misinformation often have to discard up to 50% of domains in these human-curated lists, highlighting a possibility for significant bias in collected results. For example, past research [@hounsel2020identifying] found that in a curated set of 758 disinformation websites, 575 (76%) were no longer available and had to be manually reconstructed using historical snapshots. While it is clear that persistence is a problematic assumption, we do not know to what extent this is an issue, nor do we know whether ephemerality and inaccessibility are systematic.

## Quantifying Ephemerality and Inaccessibility on the internet

Some applied studies have already dealt with URL ephemerality and inaccessibility. For example, past research [@bastos2019brexit], in analyzing URLs on Twitter, found that over 50% of the hyperlinks they examined were “Dead links” to external (non-Twitter) websites. Using actual web pages that a representative panel of American adults visited, we re-engage with scholarly work on quantifying the internet's persistent, ephemeral, and inaccessible states. 

This quantification is vital to social scientists studying human behavior on the internet because this content may not be randomly distributed across the persistent, ephemeral, and inaccessible categories. If the distribution is random, there would be less concern. However, a biased distribution would skew findings from internet researchers towards only the information they could collect, likely just the persistent content, without fully considering the ephemeral and inaccessible content. This bias is even likely given the examples above of Fast Flux Domain Networks and Paywall Journalism. Linguistics already grapples with this systematic concern by acknowledging a bias toward studying written, persistent, persistent language over spoken, ephemeral, communication [@linell2004written]. We seek to examine these potential sources of error for scholars studying content exposure on the internet and document the extent of these possible biases. We consider this bias on two of the most common objects of study on the internet: exposure to hard news and misinformation websites. 

Specifically, we ask three research questions:

**RQ1**: To what extent are hard news and misinformation website visits persistent, ephemeral, and inaccessible?

**RQ2**: Are there systematic biases in the websites and types of websites that are persistent, ephemeral, and inaccessible?

**RQ3**: Why may these biases exist?

# Data, Measures, and Methods

## Data

The data for this project come from a two-wave online survey administered via YouGov during the 2020 election to 1,515 participants. We passively gathered web browsing data (i.e., URLs) from those participants using YouGov’s Pulse browser plugin from August 24, 2020, to December 7, 2020. All participants consented to the terms of the research, and YouGov compensated the participants. We collected survey responses and URL-level tracking data from 1,238 participants. These participants visited approximately 21 million websites throughout our data collection period. 

## Measures

We narrowed our list of 21 million visited URLs to websites that are hard news, as defined by Baksy et al. [-@bakshy2015exposure] and NewsGuard^[newsguardtech.com], and misinformation websites, as categorized by Moore et al. [-@moore2022exposure]. We assigned ideological labels to websites using NewsGuard’s rating and classifications from Baksy et al. [-@bakshy2015exposure]. In addition, we only examined URLs that were to content webpages, i.e., we removed URL visits to pages such as home pages that are not specific pieces of content in an attempt not to consider dynamic web pages and removed the query parameters (i.e., site-specific data embedded in the URL) from the URLs. Some commonly visited domains that are were generally home pages, contained mostly sports content, or were labeled as partisan but ostensibly are not (e.g., websites that report the weather), were not included in the calculations^[These sites included: msn.com, news.yahoo.com, en.wikipedia.org, finance.yahoo.com, sports.yahoo.com, m.youtube.com, profootballtalk.nbcsports.com, bleacherreport.com, theringer.com, espn.com, weather.com, accuweather.com, vimeo.com, soccer.nbcsports.com, whitehouse.gov]. These steps left us with 107,783 unique URLs. 

## Method

One year after collecting URL logs, we visited each web page using a headless Google Chrome web browser. We did this to most closely simulate the real-world browsing experience of end-users using an Internet browser. We labeled the URLs we could not connect to at all as ephemeral. We labeled websites that responded to our request, but returned some sort of error code (e.g., 404 page not found) as inaccessible. 

We then placed each URL into one of three buckets: ephemeral, inaccessible, or persistent. We labeled the URLs where the browser itself crashed when trying to connect, and we received no response data as ephemeral. For the inaccessible category, we sought to identify URLs that did return content but were not the content the end-user originally observed, for example, the content behind a paywall or login form.  

To identify such content, we trained a machine learning classifier that could discern between inaccessible content and persistent content. For our training data, we hand-coded a random subset of 9,636 webpages (IRR, Cohen’s Kappa = .85) that returned content for whether the page contained a message restricting access (e.g., “This page is not available right now.”) and did not return the original content. We then used this hand-coded set to fine-tune a publicly available Huggingface BERT classifier to identify inaccessible vs. accessible content. Of the 9,636 hand-coded web pages, we used 7,724 for the training set, 1,405 for the test set, and 507 for the validation set. The model achieved high accuracy with an F1 score of 0.92 on the validation set. After applying this model to the entire set, we categorized any remaining sites that returned an error message as inaccessible. We labeled the remaining sites as persistent. Out of our 107,783 initial sites, we categorized 165 (.15%) as ephemeral, 1,838 (1.7%) as inaccessible, and 102,804 (98.1%) as persistent. 

We employ a standard chi-squared test on the distributions of persistence, inaccessibility, and ephemerality of various categories of websites (e.g., liberal misinformation websites). The top-line results for RQ1 are in Table 1, and the heterogeneous results for RQ2 are in Table 2. 


# Results


```{r setup, include=FALSE}
knitr::opts_chunk$set(
	echo = FALSE,
	message = FALSE,
	warning = FALSE
)
options(scipen = 99999)
library(tidyverse)
```

```{r}
url_list_new <- read_csv("data/url_list.csv")
```


To answer RQ1, quantifying rates of ephemerality and inaccessibility, we find low rates of ephemerality (Table 1): only 0.1% of hard news websites and 0.9% of misinformation webpages are ephemeral. We also find low, albeit higher than ephemerality, rates of inaccessibility, with 1.8% of hard news and 1.1% of misinformation web pages being inaccessible. We also find that these results are relatively stable over time (see Supplemental Materials A), although the percentage of URLs that are inaccessible an ephemeral does slight increase after the initial snapshot, highlighting the importance of capturing content as quickly as possible. 

```{r include = F}
url_list_new %>% 
  count(eph_type) %>% 
  mutate(per = n / sum(n))
```

```{r include = F}
url_list_new %>% 
  filter(!is.na(url_type)) %>% 
  count(url_type, eph_type, name = "x") %>% 
  xtabs(x~url_type+eph_type, data = .) %>% 
  chisq.test() 
```

\begin{center}
```{r}
url_list_new %>% 
  filter(!is.na(url_type)) %>% 
  count(url_type, eph_type, name = "x") %>% 
  group_by(url_type) %>% 
  mutate(n = sum(x),
         per = scales::percent(x / n)) %>% 
  select(-x, -n) %>% 
  pivot_wider(names_from = eph_type, values_from = per) %>% 
  mutate(url_type = if_else(url_type == "hard_news", "hard news", url_type)) %>% 
  select(url_type, Persistent, Ephemeral, Inaccessible) %>% 
  kableExtra::kbl(col.names = c("URL Category", "Persistent", "Ephemeral", "Inaccessible"),
               format = "latex", 
               booktabs = T,
               caption = "Percentage of Hard News and Misinformation URLs that are Persistent, Ephemeral, and Inaccessible") %>% 
  kableExtra::footnote(general = "$\\\\chi^2$(2) = 288.4, $p$ < .001", escape = F, threeparttable = T) 
```
\end{center}

For RQ2, which asks about the potential biases in ephemerality and inaccessibility, we find significant differences in ephemerality and inaccessibility rates in conservative versus liberal web pages (Table 2). Conservative hard news webpages are more likely to be ephemeral than liberal ones. Similarly, conservative misinformation webpages are more likely to be ephemeral and inaccessible than liberal ones. In other words, there are systematic biases in the ideological bent of the types of webpages that can be recovered for post hoc analysis.

```{r include = F}
url_list_new %>% 
  #filter(!is.na(ideology)) %>% 
  count(url_type, eph_type, ideology, name = "x") %>% 
  filter(url_type == "hard_news") %>% 
  xtabs(x~ideology+eph_type, data = .) %>% 
  chisq.test() 

url_list_new %>% 
  #filter(!is.na(ideology)) %>% 
  count(url_type, eph_type, ideology, name = "x") %>% 
  filter(url_type == "misinformation") %>% 
  xtabs(x~ideology+eph_type, data = .) %>% 
  chisq.test() 
```

\begin{center}
```{r}
url_list_new %>% 
  # filter(!is.na(ideology)) %>% 
  count(url_type, eph_type, ideology, name = "x") %>% 
  group_by(url_type, ideology) %>% 
  mutate(n = sum(x),
         per = scales::percent(x / n)) %>% 
  select(-x, -n) %>% 
  ungroup() %>% 
  pivot_wider(names_from = eph_type, values_from = per) %>% 
  mutate(url_type = if_else(url_type == "hard_news", "hard news", url_type),
         Ephemeral = replace_na(Ephemeral, "0.0%"),
         ideology = ifelse(is.na(ideology), "other", ideology)) %>% 
  arrange(url_type, ideology) %>% 
  select(-url_type) %>% 
  select(ideology, Persistent, Ephemeral, Inaccessible) %>% 
  kableExtra::kbl(col.names = c("Ideological Slant", "Persistent", "Ephemeral", "Inaccessible"),
               format = "latex", 
               booktabs = T,
               caption = "Percentage of Hard News and Misinformation URLs that are Persistent, Ephemeral, and Inaccessible by Ideological Slant of Website") %>% 
  kableExtra::footnote(general = "Hard news webpages: $\\\\chi^2$(3) = 27.2, $p$ < .001; Misinformation webpages: $\\\\chi^2$(3) = 9.7, $p$ = .008",
                       escape = F, threeparttable = T) %>% 
  kableExtra::pack_rows("hard news", 1, 3) %>% 
  kableExtra::pack_rows("misinformation", 4, 6)
```
\end{center}

Furthermore, specific domains are more likely to be ephemeral or inaccessible. As seen in Fig. 2, some websites are almost entirely ephemeral or inaccessible, and some domains’ webpages are a mix between ephemeral, inaccessible, and persistent. Said another way, there are misinformation and hard news websites that are systematically difficult for researchers to record the content of.

```{r fig.width = 8, fig.height = 5, fig.cap="Graph of the top five hard news and misinformation websites that are ephemeral, inaccessible, and persistent On the x-axis the percentage of the URLs from the given domain that fell into the category."}
url_list_new %>% 
  #filter(!is.na(ideology)) %>% 
  count(combined_name, url_type, ideology, eph_type, sort = T) %>% 
  group_by(combined_name) %>% 
  mutate(per_site = n / sum(n),
         ideology = if_else(is.na(ideology), "other", ideology)) %>% 
  arrange(desc(per_site)) %>% 
  group_by(url_type, eph_type) %>% 
  top_n(5, n) %>% 
  ungroup() %>% 
  group_by(url_type, eph_type) %>% 
  mutate(combined_name = fct_reorder(combined_name, per_site)) %>% 
  mutate(url_type = if_else(url_type == "hard_news", "Hard News",
                            if_else(url_type == "misinformation", "Misinformation", url_type))) %>% 
  ggplot(aes(combined_name, per_site, color = ideology)) +
  geom_point() +
  scale_y_continuous(labels = scales::percent_format()) +
  scale_color_manual(values = c("darkred", "darkblue", "grey80")) +
  coord_flip() +
  labs(title = "Top 5 Hard News and Misinformation Websites",
       subtitle = "Most Ephemeral, Inaccessible, and Persistent",
       x = "",
       y = "% in Category") +
  facet_wrap(eph_type ~ url_type, scales = "free_y", ncol = 2) + 
  theme_bw()
```

## Ephemeral Website Error

To answer RQ3, why some websites are ephemeral, we turn to the technical details of the URLs we attempted to scrape. Websites may not return content at requested URLs for a myriad of reasons – for example, the page may no longer exist, the website may no longer exist, or changes to the website’s configuration may render the page no longer accessible. We track and aggregate the top errors thrown by our crawling infrastructure for ephemeral URLs in Table 3. The most common reason a site is ephemeral is due to a timeout (33.4%). We stay on a single webpage for up to 60 seconds as a timeout window – after this point, we determine there is likely some unknown external factor (e.g., web server configuration error) that is preventing us from retrieving the page. 

Other errors are more specific – 23.5% of ephemeral URLs failed due to an HTTP Error, which typically meant the server response failed in an unrecoverable way. While it is hard to ascertain exactly why a website operator would take down a website, it highlights that many such ephemeral articles are removed over time, and post hoc analyses would miss these URLs. Other errors point to fundamental web server failures – 9.9% of errors are Name Not Resolved, meaning that the domain itself has been taken down since the YouGov Pulse data was collected. Our examination of these errors suggests a complex set of factors that dictate URL ephemerality. 

\begin{center}
```{r}
tribble(~"Error Type", ~"Percentage", ~"Explanation",
        "Timeout", "33.4%", "The webpage did not load after one minute of waiting",
        "HTTP Failure", "23.5%", "The webpage returned an error that rendered the browser unable to continue",
        "Name Not Resolved", "9.9%", "The domain name for this URL no longer exists",
        "Cert Common Name Invalid", "7.5%", "The website no longer has a valid HTTPS certificate.",
        "Connection Aborted", "5.7%", "The connection was stopped by the server.",
        "Connection Refused", "3.9%", "The server refused to respond to the request issued by the browser",
        "Certificate Date Invalid", "2.5%", "The website no longer has a valid HTTPS certificate",
        "SSL Protocol Error", "2.4%", "The server failed to properly establish an HTTPS connection.") %>% 
  kableExtra::kbl(format = "latex", 
               booktabs = T,
               caption = "Ephemeral URL Errors") %>% 
  kableExtra::footnote(general = "Breakdown of the percentage of ephemeral URLs that returned each error code.",
                       escape = F, threeparttable = T) %>% 
  kableExtra::column_spec(3, width = "20em")
```
\end{center}

## Inaccessible Website Errors

A significant number of websites are inaccessible, meaning that the content of the page either sits behind a paywall or is no longer available. To better characterize these errors, we investigated the most common HTTP status codes for inaccessible websites (Table 4). The majority of status codes (56.7%) were 404 not found, which means the original page has gone missing and no longer appears on the server. The second most prominent error was a 200 OK (18.9%), which means that while the page was accessible by our crawler, the content of the page was behind a paywall. Other, less common errors include 500 (10.4%), 403 (6.3%), 410 (4.8%), and 400 (2.4%), all of which in some way either restrict access to the webpage or render it unavailable.

\begin{center}
```{r}
tribble(~"Error Code", ~"Percentage", ~"Explanation",
        404, "56.7%", "404 means missing page – which could mean the page has disappeared from the server",
        200, "18.9%", "Page returned content, but typically contains a paywall or is otherwise inaccessible",
        500, "10.4%", "Unexpected Server Error",
        403, "6.3%", "403 means the page has forbidden access; the user may have had privilege that we do not have as researchers",
        410, "4.8%", "Resource has vanished and it is unlikely to come back (GONE)",
        400, "2.4%", "Server will not process the request") %>% 
  kableExtra::kbl(format = "latex", 
               booktabs = T,
               caption = "Inaccessible URL Error Codes") %>% 
  kableExtra::footnote(general = "Breakdown of the percentage of inaccessible URLs that returned each error code.",
                       escape = F, threeparttable = T) %>% 
  kableExtra::column_spec(3, width = "20em")
```
\end{center}

# Discussion

The present study examined the operative status–the persistence, inaccessibility, and ephemerality–of scraped websites in a nationally representative sample of American adults’ web browsing during the 2020 U.S. Presidential Election. We find that hard news websites are more likely than misinformation websites to be inaccessible, but that misinformation websites are generally more likely to be ephemeral. When looking at the ideological slant of the websites, conservative misinformation was the most likely to be ephemeral. Broadly, ephemeral errors are due to misconfigured servers that either never return any content or, in some cases, cease to exist on the Internet altogether. Inaccessible errors often stem from paywalls, however, in some cases, websites may restrict certain articles or take them down altogether. 

These results have implications for misinformation research. Considering that misinformation is relatively rare [@dahlke2022mixed; @guess2020exposure; @moore2022exposure], each piece of misinformation exposure is important. Misinformation researchers should work to document the content of misinformation as quickly as possible after its creation or exposure in order to preserve and study its contents. In particular, researchers should consider that some types of misinformation may be systematically more difficult to capture and either make special efforts to collect that content or consider the implications of potentially missing it. 

The present research, however, has much broader implications for any researcher conducting web scraping. In particular, we have identified three key metrics that quantify potential error associated with a web page's operative status. We encourage future research that uses scraped web data to report the $PIE$ metrics. These metrics are: $PersistenceRate = \frac{p}{t}$ where $p$ is the number of persistent web pages and $t$ is the number of total web pages scraped; $InaccessibilityRate = \frac{i}{t}$ where $i$ is the number of inaccessible web pages; and $EphemeralityRate = \frac{e}{t}$ where $e$ is the number of ephemeral web pages.  

Then, these $PIE$ metrics should be reported in a consistent manner through a table, as modeled in Table 5, where there are at least two categories of websites (Type A, Type B, Type C, etc.). This sort of test is flexible to handle granular levels, for example, even down to the web-domain level. For data with nested subgroups, we recommend a table such as Table 2. Crucially, we recommend a chi-squared test of the distributions to determine if the distribution of the $PIE$ metrics significantly differ across subgroups. If the distributions are significantly different, that suggests that there is systematic bias in one’s data. 

\begin{center}
```{r}
tribble(~"Category", ~"Persistent", ~"Inaccessible", ~"Ephemeral",
        "Type A", "__%", "__%", "__%",
        "Type B", "__%", "__%", "__%") %>% 
  kableExtra::kbl(format = "latex",
               booktabs = T,
               caption = "PIE Table Template") %>% 
                 kableExtra::footnote(general = "Type A webpages: $\\\\chi^2$(\\\\_\\\\_) = \\\\_\\\\_, $p$ = \\\\_\\\\_; Type B webpages: $\\\\chi^2$(\\\\_\\\\_) = \\\\_\\\\_, $p$ = \\\\_\\\\_",
                       escape = F, threeparttable = T)
```
\end{center}
When this test is significant–and thus the data show systematic bias–we recommend that authors should do three things: 1) authors should consider whether this bias compromises their results or requires other methods to overcome the bias (e.g., recover ephemeral sites via an online archive), 2) conduct an error analysis to examine why some categories’ $PIE$ metrics are different, and 3) note in the limitation of the study that there is potential bias that could influence inferences from the analysis. We note that there is no perfect sampling of websites, in the same way that sampling of human participants in studies is never perfectly representative of the underlying population. Therefore, similarly to how sampling metrics are always reported in human participant studies, we argue here that the $PIE$ metrics should always be reported for web scraping studies to give readers an understanding of the potential biases in a study’s data. Hopefully, future meta-analytic work can use these standardized metrics to gain a more holistic understanding of the distribution of $PIE$ metrics across the internet and websites of interest to scholars. 

# Conclusion

We examine the persistence, inaccessibility, and ephemerality of web scraping data from web browsing data of all misinformation and hard news websites that 1,515 individuals visited across 21M URL-level visits. We find significant amounts of systematic bias in the scraped data. Misinformation, particularly conservative misinformation, web pages are more likely to be ephemeral. Hard news, specifically liberal hard news, web pages are more likely to be inaccessible. We suggest that future researchers should take care to consider and report the systematic biases in their own data by reporting the $PIE$ metrics, $PersistenceRate$, $InaccessibilityRate$, and $EphemeralityRate$ in a standard way that makes clear the potential biases in one’s data and allows for easy interpretation across studies. 

\newpage

\renewcommand{\appendixname}{Supplementary Material}
\renewcommand{\thefigure}{S\arabic{figure}} \setcounter{figure}{0}
\renewcommand{\thetable}{S\arabic{table}} \setcounter{table}{0}
\renewcommand{\theequation}{S\arabic{table}} \setcounter{equation}{0}

# Supplemental Materials

## A. Stability of Results Over Time

We also tested the time stability of our results (see Figure S1). To do so, we repeated the same process outlined above at three different time periods post data collection: at one year, one-and-a-half years, and two full years. The rates of inaccessible websites and ephemeral websites slightly increase after the initial snapshot; this is likely because many websites (especially news websites) will transition older articles to archived content. However, it does highlight the importance of collecting content-related information as quickly as possible to the data collection date, as some content has a short life cycle on the Internet. Future work should examine not only which websites are likely to be inaccessible and ephemeral but also how these biases may be exacerbated as time goes on. 

```{r}
urls_eph_time <- read_csv("data/urls_eph_time.csv")
```


```{r fig.cap="Percentage of URLs in our data set that are persistent, inaccessible, and ephemeral over time. On the x-axis is the snapshot number. Snapshot #1 was conducted one year after data collection. Snapshot #2 was conducted one-and-a-half years after data collection. Snapshot #3 was conducted two years after data collection." }
urls_eph_time %>% 
  count(eph_type, snapshot) %>% 
  group_by(snapshot) %>% 
  mutate(per = n / sum(n),
         label = ifelse(snapshot == 1, eph_type, NA)) %>% 
  ggplot(aes(snapshot, per, color = eph_type, label = label)) +
  geom_point(size = 4) +
  geom_line(size = 1.5) +
  scale_y_continuous(labels = scales::percent_format()) +
  scale_x_continuous(breaks = c(1, 2, 3)) +
  ggsci::scale_color_npg() +
  # coord_cartesian(ylim = c(0, 1)) +
  ggrepel::geom_label_repel(box.padding = .5) +
  labs(title = "PIE metrics over time",
       x = "Snapshot #",
       y = "percentage") +
  theme_bw() +
  theme(legend.position = "none") +
  facet_wrap(~eph_type, ncol = 1, scales = "free") +
  expand_limits(y = 0)
```

\newpage

# References